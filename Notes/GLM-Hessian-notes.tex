\PassOptionsToClass{
  title={GLM Hessian Notes},
  header,
  letterpaper,
  12pt
}{hw-scrartcl}

\documentclass{hw-scrartcl}

\usepackage{prob-style}
\usepackage{my-theorems}

\begin{document}
\maketitle
\section{Definitions and set-up}
We will consider a glm to be a collection of samples \(\{(x_i, y_i)\}_{i=1}^n \subseteq \R^p \times \R\), where
\[
  y_i|x_i
  \sim P_{x_i \transp \beta}
\]
for \(\{P_\eta | \eta \in H\}\) a \(1\)-parameter exponential family indexed by a natural parameter \(\eta\) with densities
\[
  p_\eta(y)
  = \exp\{\eta y - \psi(\eta)\} p_0(y).
\]

Conditional on \(X\), the matrix whose rows are the \(x_i\)s, \(y\) thus has density
\[
  f_\beta(y)
  = \exp\Big\{\beta\transp (X\transp y) - \sum_{i=1}^n \psi(x_i\transp \beta)\Big\} f_0(y).
\]

The likelihood and corresponding derivatives are then given by
\begin{align*}
  \ell(\beta)
  &= \beta\transp (X\transp y) - \sum_{i=1}^n \psi(x_i\transp \beta) + \log f_0(y), \\
  \nabla \ell(\beta)
  &= X\transp y - \sum_{i=1}^n \psi'(x_i\transp \beta) x_i, \\
  \nabla^2 \ell(\beta)
  &= -\sum_{i=1}^n x_i \psi''(x_i\transp \beta) x_t \transp \\
  &= -X\transp D_{X\beta} X,
\end{align*}
where \(D_{X\beta}\) is the diagonal matrix with \(i\)th entry \(\psi''(x_i\transp \beta)\), which is the conditional variance of \(y_i\).

Notice in particular that the Hessian of the log-likelihood has no dependence on \(y\) --- a feature unique to GLMs.

\section{The Hessian at the MLE}
We are interested in the spectrum of \(\nabla^2 \ell(\hat{\beta})\). Notice from the previous remark that this depends on \(y\) only through \(\hat{\beta}\). In particular, if \(\hat{\beta}\) is close to the true \(\beta_0\), we can expect that \(\nabla^2 \ell(\hat{\beta})\) is close to \(\nabla^2 \ell(\beta_0)\), and so doesn't depend on \(y\). In this case, the Hessian would be unaffected by using \(y\) to learn \(\beta\).

To show this convergence, we will study the distance between eigenvalue distributions of matrices of the form \(X\transp D X\). In particular, for any Lipschitz function \(f\), we have that
\begin{align*}
  \abs[\Big]{\int f(\lambda) \diff \mu_u(\lambda) - \int f(\lambda) \diff \mu_v(\lambda)}
  &\leq \frac{1}{n} \sum_{i=1}^n \abs{f(\lambda_i) - f(\nu_i)} \\
  &\leq \frac{1}{n} \sum_{i=1}^n \abs{\lambda_i - \nu_i} \\
  &\leq \frac{1}{n} \norm{X\transp (D_u - D_v) X}_1 \\
  &\leq \frac{1}{n} \norm{D_u - D_v}_1 \norm{X}_\infty^2 \\
  &= \norm{u - v}_1 \frac{1}{n} \norm{X}_\infty^2.
\end{align*}

If \(X\) is an \(n \times p\) matrix of iid normals, \(n\rightarrow \infty\) with \(p\) fixed, we have that \(\norm{X}_\infty^2 \sim n\) and \(\norm{\hat{\beta} - \beta_0} \rightarrow 0\). In this case, we have that
\[
  \mu_{\hat{\beta}} \rightarrow \mu_{\beta_0}
\]
almost surely in Wasserstein distance (and hence, for example, weakly).

\section{Logistic regression}
Consider the logistic setting where \(y_i\) takes values \(\{\pm 1\}\). In this case, we have that
\[
  \psi''(\eta)
  = \frac{1}{\cosh^2 \eta},
\]
and therefore
\[
  \nabla^2 \ell(\beta)
  = -\sum_{i=1}^n \frac{x_i x_i\transp}{\cosh^2 x_i\transp \beta}.
\]

For simplicity, consider the case where \(x_{ij}\) are iid normals and \(\beta \in \Unif(S^{p-1})\) is constant. Let \(P = I - \beta\beta\transp\) be the projection onto the subspace orthogonal to \(\beta\). We can then write
\[
  \nabla^2 \ell(\beta)
  = -\sum_{i=1}^n \frac{(P x_i) x_i\transp}{\cosh^2 x_i\transp \beta} - \beta \sum_{i=1}^n \frac{(\beta\transp x_i) x_i\transp}{\cosh^2 x_i\transp \beta}.
\]

Asymptotically, the second

\section{Broad strokes argument for semicircle law for small covariance matrices}
First, notice that for a matrix \(A\) and scalar \(\lambda\), we have that
\begin{align*}
  s_{\lambda A}(z)
  &= \frac{1}{d} \Tr(\lambda A - z\Id)^{-1} \\
  &= \lambda^{-1}\frac{1}{d} \Tr(A - (z/\lambda) \Id)^{-1}\\
  &= \lambda^{-1} s_A(z \lambda^{-1}).
\end{align*}

From this it follows that
\[
  z_{\lambda A}(s) = \lambda z_A(\lambda s),
  \qquad
  R_{\lambda A}(s) = \lambda R_A(\lambda).
\]

Now, we have that, asymptotically, \(\norm{x_i}^2 \sim p\). Hence, we write that
\begin{align*}
  s_{x_i x_i\transp/p}(z)
  &= \frac{1}{p}\Tr\Big(\frac{1}{p}x_i x_i\transp - z\Id\Big)^{-1} \\
  &= \frac{1}{p}\Big[\frac{1}{\norm{x_i}^2/p - z} - \frac{p-1}{z}\Big] \\
  &\approx \frac{1}{p}\Big[\frac{1}{1 - z} - \frac{p-1}{z}\Big].
\end{align*}

For large \(p\), it follow that then
\begin{align*}
  z_{x_i x_i\transp/p}(s)
  &\approx -\frac{1}{s} + \frac{1}{p(1 + s)}, \\
  R_{x_i x_i\transp/p}(s)
  &\approx \frac{1}{p(1 - s)}.
\end{align*}

Assuming the corresponding convolution is indeed asymptotically free, we have that
\begin{align*}
  R_{\sum_{i=1}^n x_i x_i\transp/p}(s)
  &\approx \frac{n}{p(1-s)}, \\
  R_{\frac{1}{2\sqrt{np}}\sum_{i=1}^n x_i x_i\transp}(s)
  &= \frac{1}{2}\sqrt{\frac{p}{n}} R_{\sum_{i=1}^n x_i x_i\transp/p}\Big(\frac{1}{2}\sqrt{\frac{p}{n}}s\Big) \\
  &\approx  \sqrt{\frac{n}{p}}\frac{1}{2 - s\sqrt{n/p}}.
\end{align*}

Finally, since \(R_I(z) = 1\), we have that
\begin{align*}
  R_{\frac{1}{2\sqrt{np}}(\sum_{i=1}^n x_i x_i\transp - nI)}(s)
  &\approx \sqrt{\frac{n}{p}}\frac{1}{2 - s\sqrt{p/n}} - \frac{1}{2}\sqrt{\frac{n}{p}} \\
  &= \sqrt{\frac{n}{p}} \frac{s\sqrt{p/n}}{2(2 - s\sqrt{p/n})}\\
  &= \frac{s}{2(2 - s\sqrt{p/n})} \\
  &\approx \frac{s}{4}.
\end{align*}

Indeed, this is the \(R\)-transform of the semicircle law on \([-1, 1]\). Notice that is seems that the \(x_i\) having internal dependence causes no problems, except possibly rendering the sum asymptotically free as long as \(\norm{x_i}^2 \sim p\).
\end{document}
